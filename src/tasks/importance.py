import logging
from enum import Enum
from typing import Union, Iterable, Optional

import numpy as np
from pydantic import BaseModel, Field
from scipy.stats import rankdata
from torch import tensor
from torch.nn.functional import softmax

from src.data.instance import Instance
from src.llm.manager import TaskManager, FieldDefinition, SamplingParams
from src.scoring import ConfidenceMethod, MethodFactory

logger = logging.getLogger(__name__)

LOGIT_IMPORTANCE_WEIGHTS = tensor([2.0, 1.0, -1.0])


class SupportScore(int, Enum):
    SUPPORT = 1
    CONTRADICT = -1
    UNRELATED = 0


class ImportanceAnnotation(str, Enum):
    IMPORTANT = 'Y'
    UNIMPORTANT = 'N'
    UNKNOWN = 'N/A'

    def is_important(self):
        return self == ImportanceAnnotation.IMPORTANT


class ScoredImportanceClaim(BaseModel):
    """
    A claim with a confidence score generated by the model.
    """
    claim: str = Field(description="The claim to be scored.")
    confidence: float = Field(
        description="Importance score in the claim, where 1 is very important and 0 is not important."
    )


class ImportanceSubclaim(BaseModel):
    """
    Represents a subclaim with associated confidence scores derived from multiple evaluation methods.

    This model stores various confidence scores assigned to a subclaim based on different methodologies.
    These scores help quantify the reliability or factual accuracy of the subclaim using approaches such as
    random scoring, baseline heuristics, GPT-based evaluation, frequency-based scoring, and optimal labeling.
    """

    subclaim: str = Field(alias='subclaim', description="The textual representation of the subclaim.")
    scores: dict[str, Union[float, None]] = Field(default_factory=dict)
    label: ImportanceAnnotation = Field(alias='annotation', default=ImportanceAnnotation.UNKNOWN)
    noise: float = Field(default_factory=lambda: np.random.normal() * 0.001)


class ImportanceInstance(Instance):
    text: str
    claims: list[ImportanceSubclaim]
    summary: Optional[str] = None  # Reference summary text

    @property
    def noise(self) -> np.ndarray:
        return np.asarray([claim.noise for claim in self.claims])

    @property
    def labels(self) -> np.ndarray:
        return np.asarray([claim.label.is_important() for claim in self.claims])
    
    def get_available_methods(self) -> list[str]:
        cached_methods = self.claims[0].scores.keys()
        registered_methods = MethodFactory.available_methods()
        return list(set(cached_methods).union(set(registered_methods))) 

    def get_confidence_score(self, method: ConfidenceMethod | str, ranking: bool, noise: bool = True) -> np.ndarray:
        """
        Computes the factuality score R(c) for each subclaim using the specified method.

        :param method: ConfidenceMethod to use as the subclaim scoring function.
        :param ranking: Whether to use ranking or not.
        :param noise: Whether to add noise to the scores.
        :return: Array of factuality scores.
        """
        if len(self.claims) == 0:
            return np.empty((0,), dtype=float)

        if isinstance(method, str):
            if method in self.claims[0].scores:  # Cached
                scores = np.asarray([item.scores[method] for item in self.claims], dtype=float)
            else:
                subclaims = [claim.subclaim for claim in self.claims]
                method_instance = MethodFactory.create(method)
                scores = method_instance.compute(self.text, subclaims, self.summary).astype(float)
                for item, score in zip(self.claims, scores):
                    item.scores[method] = score
        else:
            subclaims = [claim.subclaim for claim in self.claims]
            scores = method.compute(self.text, subclaims, self.summary).astype(float)
            if hasattr(method, "name"):
                for item, score in zip(self.claims, scores):
                    item.scores[method.name] = score

        if noise:
            scores += self.noise

        if ranking:
            # Add noise to break ties
            rankings = rankdata(-scores, method='ordinal')
            return (len(scores) + 1 - rankings) / len(scores)
        else:
            return scores

    def get_accepted_subclaims(
            self, method: ConfidenceMethod, ranking: bool, threshold: float
    ) -> list[ImportanceSubclaim]:
        """
        Get the claims with an importance score R(c) above the threshold.

        :param method: ConfidenceMethod to use as the subclaim scoring function.
        :param ranking: Whether to use ranking or not.
        :param threshold: Threshold for the confidence score.
        :return: List of accepted subclaims.
        """

        scores = self.get_confidence_score(method, ranking, noise=True)
        accepted_subclaims = [claim for claim, score in zip(self.claims, scores) if score >= threshold]
        return accepted_subclaims

    def get_conformal_score(self, method: ConfidenceMethod, ranking: bool, beta: float) -> float:
        """
        Compute the conformal score for a dataset instance when method is used as the subclaim scoring function.

        Full entailment: S(xᵢ, yᵢ) :=sup {q ∈ R⁺ | ∀q′ <= q, ∀c s.t. W(c, xᵢ, yᵢ) = 1, c ∈ Fq′(x̂ᵢ)}
                                   := sup {q ∈ R⁺ | ∀q′ <= q, [∑₍c∈F₍q′₎(x̂ᵢ)₎ W(c,xᵢ,yᵢ) / ∑₍c∈C₎ W(c,xᵢ,yᵢ)] = 1 }
        Partial entailment: Sₐ(xᵢ, yᵢ) := sup {q ∈ R⁺ | ∀q′ <= q, [∑₍c∈F₍q′₎(x̂ᵢ)₎ W(c,xᵢ,yᵢ) / ∑₍c∈C₎ W(c,xᵢ,yᵢ)] ≥ a }

        This is equivalent to raising the threshold until the beta of important claims out of all claims is greater
        than or equal to the beta, which is similar to recall.

        When beta = 1, the threshold is the smallest threshold q such that all important claims are retained.
        When beta = 0, the threshold is -inf as no retained claims are required to be important.

        :param method: ConfidenceMethod to use as the subclaim scoring function.
        :param ranking: Whether to use ranking or not.
        :param beta: Fraction of important subclaims required (a in formula above).
        :return: Conformal score for the dataset instance.
        """

        confidence_scores = self.get_confidence_score(method, ranking=ranking)

        sort_idx = np.argsort(-confidence_scores)  # sort in descending order
        sorted_confidence_scores = confidence_scores[sort_idx]

        importance = self.labels[sort_idx]
        total_important = np.sum(importance)

        # Edge cases:
        # - All claims are important (beta = 1): highest confidence score
        # - All claims are not important (beta = 1): -inf
        # - No claims are required to be important (beta = 0): -inf

        if total_important == 0:
            return np.inf

        important_fractions = np.cumsum(importance) / total_important
        indices = np.asarray(important_fractions >= beta).nonzero()[0]
        return float(sorted_confidence_scores[indices[0]] if indices.size > 0 else np.inf)

    @classmethod
    def from_row(cls, row: dict, label_col: str = "input_sentences_labels"):

        def claims_from_row(row):
            sentences = row["input_sentences"]
            method_scores = dict()
            labels = [ImportanceAnnotation.IMPORTANT if item else ImportanceAnnotation.UNIMPORTANT for item in
                      row[label_col]]
            for col in row:
                if col.endswith("scores"):
                    method_name = col.rsplit("scores", 1)[0].rstrip("_")
                    method_scores[method_name] = row[col]

            claims = [ImportanceSubclaim(subclaim=sentences[i], annotation=labels[i],
                                         scores={method: method_scores[method][i] 
                                                 for method in method_scores if method_scores[method]}) 
                                                 for i in range(len(sentences))]
            return claims

        return cls(text=row["input"], claims=claims_from_row(row), summary=row["summary"])


class ImportanceTaskManager(TaskManager):
    async def get_summary(self, text: str, sampling_params: Optional[SamplingParams] = None) -> str:
        prompt = f"""
        Summarize the following text by extracting the most important information.
        Focus on the core message, key details, and any critical takeaways.
        Ensure the summary is clear, concise, and retains the essential meaning while removing unnecessary details.
        
        text (The input text to summarize): {text}
        """
        return await self.generate_async(prompt, sampling_params=sampling_params)

    # async def get_subclaims(
    #         self, text: str, sampling_params: Optional[SamplingParams] = None
    # ) -> list[ScoredImportanceClaim]:
    #     prompt = f"""
    #     Break down the following text into a set of small, independent claims that are easy to verify by a human.
    #     Each atomic fact should be a short, non-overlapping, verifiable statement that conveys a single piece of information.
    #     Atomic subclaims are more granular than full sentences and are used for fine-grained evaluation.

    #     text (The input text containing one or multiple claims): {text}
    #     """
    #     response = await self.generate_async(prompt, [
    #         FieldDefinition(
    #             name='subclaims', field_type=list[ScoredImportanceClaim],
    #             description="List of independent atomic subclaims with confidence scores."
    #         )
    #     ], sampling_params=sampling_params)
    #     return response.subclaims

    async def score_subclaims(
            self, subclaims: list[str], sampling_params: Optional[SamplingParams] = None
    ) -> list[ScoredImportanceClaim]:
        instruction = f"""
        Generate a list of scores for each claim indicating whether the claim is important or not.
        
        """
        prompt = '\n'.join([instruction, 'subclaims (A list of subclaims)'] + subclaims)

        response = await self.generate_async(prompt, [
            FieldDefinition(
                name='scores', field_type=list[float],
                description="A list of importance scores for each claim, where 1 is very important and 0 is not important."
            )
        ], sampling_params=sampling_params)

        scores = response.scores

        if len(scores) != len(subclaims):
            logger.info('Length of output scores does not match subclaims, falling back to individual queries')
            scores = []

            for claim in subclaims:
                prompt = '\n'.join([instruction, f'claim (A given subclaim): {claim}'])

                response = await self.generate_async(prompt, [
                    FieldDefinition(
                        name='score', field_type=float,
                        description="Importance score of the claim, represented as a continuous value between 0 and 1, "
                                    "where 1 indicates very high importance and 0 indicates no importance."
                    )
                ], sampling_params=sampling_params)
                scores.append(response.score)

        return [ScoredImportanceClaim(claim=claim, confidence=score) for claim, score in zip(subclaims, scores)]

    async def get_logit_scores(
            self, subclaims: list[str], sampling_params: Optional[SamplingParams] = None
    ) -> list[ScoredImportanceClaim]:

        instruction = f"""Please evaluate the importance of each input claims in the following list, 
        based on how the information carried in the claim is aligned with the overall message. 
        Please provide a numbered list of importance scores for EACH input claim.
        Each output score should be one of High, Medium, or Low, indicating how important the corresponding
        input claim is in the context of the text document.
        """
        if sampling_params.logprobs is None:
            raise ValueError("Logprobs must be set to True for logit scores")
        prompt = '\n'.join(
            [instruction, 'The claims are:'] +
            [f"{i + 1}. {claim}" for i, claim in enumerate(subclaims)]
        )
        full_response = await self.generate_async(prompt, response_format=None, sampling_params=sampling_params)

        # Filter out parts of the response not corresponding to scores
        response = []
        for logprob_dict in full_response:
            max_token = max(logprob_dict, key=lambda x: logprob_dict[x])
            if max_token not in ["High", "Medium", "Low"]:
                continue
            response.append(logprob_dict)

        if len(response) != len(subclaims):
            logger.info('Length of output scores does not match subclaims, falling back to individual queries')
            response = []

            for claim in subclaims:
                full_text = "\n".join(subclaims)
                prompt = f"""Decide if the following claim's importance to the entire text is Low, Medium, or High. 
                Respond with just the single word for the score\nThe text: {full_text}
                The claim: {claim}"""

                claim_response = await self.generate_async(prompt, response_format=None,
                                                           sampling_params=sampling_params)
                response.extend(claim_response)

        claims = []
        assert len(subclaims) == len(
            response), f"Length of subclaims ({len(subclaims)}) does not match length of response ({len(response)})"
        for claim, response_item in zip(subclaims, response):
            high_logit = response_item.get("High", 0)
            medium_logit = response_item.get("Medium", 0)
            low_logit = response_item.get("Low", 0)
            # Convert to probabilities
            probabilities = softmax(tensor([high_logit, medium_logit, low_logit]), dim=0)

            claim = ScoredImportanceClaim(
                claim=claim,
                confidence=LOGIT_IMPORTANCE_WEIGHTS @ np.array(probabilities)
            )
            claims.append(claim)
        return claims

    async def get_relevance(
            self,
            subclaims: list[Union[str, ScoredImportanceClaim]],
            text: str,
            sampling_params: Optional[SamplingParams] = None
    ) -> list[SupportScore]:

        # Override temperature until Enum support is enabled
        if sampling_params is None:
            sampling_params = SamplingParams(temperature=0.0)
        else:
            sampling_params = sampling_params.copy()
            sampling_params.temperature = 0.0

        instruction = f"""
        For each claim, score whether the text supports, contradicts, or is unrelated to the claim.

        Ensure the number of claims and scores are equal.
        
        text (An input text): {text}
        
        """
        subclaims = [item.claim if isinstance(item, ScoredImportanceClaim) else item for item in subclaims]
        prompt = '\n'.join([instruction, 'subclaims (A list of subclaims)'] + subclaims)

        response = await self.generate_async(prompt, [
            FieldDefinition(
                name='scores', field_type=list[SupportScore],
                description="A list of scores indicating whether the text supports (1), "
                            "contradicts (-1), or is unrelated (0) to each subclaim."
            )
        ], sampling_params=sampling_params)

        scores = response.scores

        if len(scores) == len(subclaims):
            return scores

        logger.info('Length of output scores does not match subclaims, falling back to individual queries')
        scores = []

        for claim in subclaims:
            prompt = '\n'.join([instruction, f'claim (A given subclaim): {claim}'])

            response = await self.generate_async(prompt, [
                FieldDefinition(
                    name='score', field_type=SupportScore,
                    description="A score indicating whether the text supports (1), "
                                "contradicts (-1), or is unrelated (0) to the given subclaim."
                )
            ], sampling_params=sampling_params)
            scores.append(response.score)
        return scores

    async def get_labels(
            self, subclaims: Iterable[Union[str, ScoredImportanceClaim]], summary: str,
            sampling_params: Optional[SamplingParams] = None
    ) -> list[ImportanceAnnotation]:
        instruction = f"""
        For the given claim, score whether the text entails or does not entail the claim.

        text (The input text): {summary}
        """

        subclaims = [claim.claim if isinstance(claim, ScoredImportanceClaim) else claim for claim in subclaims]
        prompt = '\n'.join([instruction, 'subclaims (A list of subclaims)'] + subclaims)

        response = await self.generate_async(prompt, [
            FieldDefinition(
                name='labels', field_type=list[ImportanceAnnotation],
                description="A list of entailment labels indicating whether the text entails (Y) "
                            "or does not entail (N) each subclaim."
            )
        ], sampling_params=sampling_params)

        labels = response.labels

        if len(labels) == len(subclaims):
            return labels

        logger.info('Length of output labels does not match subclaims, falling back to individual queries')

        for claim in subclaims:
            prompt = '\n'.join([instruction, f'claim (A given subclaim): {claim}'])

            response = await self.generate_async(prompt, [
                FieldDefinition(
                    name='label', field_type=ImportanceAnnotation,
                    description="An entailment label indicating whether the text entails (Y) or does not entail (N) the claim."
                )
            ], sampling_params=sampling_params)
            labels.append(response.label)
        return labels

    # async def get_output(
    #         self, subclaims: Iterable[Union[str, ScoredImportanceClaim]],
    #         sampling_params: Optional[SamplingParams] = None
    # ) -> str:
    #     instruction = f"""
    #     You will get a set of subclaims that are important.
    #     Using the subclaims provided, generate an output that is easy to read.

    #     subclaims (A list of subclaims that are important)
    #     """

    #     subclaims = [claim.claim if isinstance(claim, ScoredImportanceClaim) else claim for claim in subclaims]
    #     prompt = '\n'.join([instruction] + subclaims)

    #     response = await self.generate_async(prompt, sampling_params=sampling_params)
    #     return response

    async def get_frequency_scores(
            self, subclaims: list[str], text: str, n_samples: int = 5,
            sampling_params: Optional[SamplingParams] = None
    ) -> list[float]:
        scores = [0] * len(subclaims)

        for _ in range(n_samples):
            summary = await self.get_summary(text, sampling_params=sampling_params)
            relevance = await self.get_relevance(subclaims, summary, sampling_params=sampling_params)

            for i, score in enumerate(relevance):
                scores[i] += float(score)
        return scores

    async def get_instance(
            self,
            text: str,
            subclaims: Optional[list[str]] = None,
            summary: Optional[str] = None,
            sampling_params: Optional[SamplingParams] = None
    ):
        output = await self.get_summary(text, sampling_params=sampling_params)
        if subclaims is None:
            subclaims = await self.get_subclaims(text, sampling_params=sampling_params)
        else:
            subclaims = await self.score_subclaims(subclaims, sampling_params=sampling_params)

        if summary is not None:
            labels = await self.get_labels(subclaims=subclaims, summary=summary, sampling_params=sampling_params)
        else:
            labels = [ImportanceAnnotation.UNKNOWN] * len(subclaims)

        claims = []
        for item, label in zip(subclaims, labels):
            claim = ImportanceSubclaim(
                subclaim=item.claim,
                scores={'gpt': item.confidence, 'optimal': label.is_important()},
                label=label,
            )
            claims.append(claim)

        return ImportanceInstance(**{
            'text': text, 'original-output': output, 'claims': claims, 'summary': summary
        })
