import logging
from enum import Enum
from typing import Union, Iterable, Optional

import numpy as np
from pydantic import BaseModel, Field, computed_field
from scipy.stats import rankdata

from src.data.instance import Instance
from src.llm.manager import TaskManager, FieldDefinition, SamplingParams
from src.scoring import ConfidenceMethod, MethodFactory

logger = logging.getLogger(__name__)


class SupportScore(int, Enum):
    SUPPORT = 1
    CONTRADICT = -1
    UNRELATED = 0


class FactualAnnotation(str, Enum):
    FACTUAL = 'Y'
    SUBJECTIVE = 'S'
    FALSE = 'N'
    UNVERIFIABLE = 'U'
    UNKNOWN = 'N/A'

    def is_correct(self):
        return self in [FactualAnnotation.FACTUAL, FactualAnnotation.SUBJECTIVE]


class ScoredFactualClaim(BaseModel):  # fact-based confidence
    """
    A factual claim with a confidence score generated by the model.
    """
    claim: str = Field(description="The claim to be scored.")
    confidence: float = Field(
        description="Confidence score in the claim, where 1 is obvious subclaims and results like "
                    "'The earth is round' and '1+1=2'. A 0 is for claims that are very obscure or difficult "
                    "for anyone to know, like the birthdays of non-notable people."
    )


class FactualSubclaim(BaseModel):
    """
    Represents a subclaim with associated confidence scores derived from multiple evaluation methods.

    This model stores various confidence scores assigned to a subclaim based on different methodologies.
    These scores help quantify the reliability or factual accuracy of the subclaim using approaches such as
    random scoring, baseline heuristics, GPT-based evaluation, frequency-based scoring, and optimal labeling.
    """

    subclaim: str = Field(alias='subclaim', description="The textual representation of the subclaim.")
    scores: dict[str, float] = Field(default_factory=dict)
    label: FactualAnnotation = Field(alias='annotation', default=FactualAnnotation.UNKNOWN)
    noise: float = Field(default_factory=lambda: np.random.normal() * 0.001)


class FactualInstance(Instance):
    prompt: str
    claims: list[FactualSubclaim]

    @computed_field
    @property
    def noise(self) -> np.ndarray:
        return np.asarray([claim.noise for claim in self.claims])

    @computed_field
    @property
    def labels(self) -> np.ndarray:
        return np.asarray([claim.label.is_correct() for claim in self.claims])

    def get_confidence_score(self, method: ConfidenceMethod | str, ranking: bool, noise: bool = True) -> np.ndarray:
        """
        Computes the factuality score R(c) for each subclaim using the specified method.

        :param method: ConfidenceMethod to use as the subclaim scoring function.
        :param ranking: Whether to use ranking or not.
        :param noise: Whether to add noise to the scores.
        :return: Array of factuality scores.
        """
        if len(self.claims) == 0:
            return np.empty((0,), dtype=float)

        if isinstance(method, str):
            if method in self.claims[0].scores:  # Cached
                scores = np.asarray([item.scores[method] for item in self.claims])
            else:
                subclaims = [item.subclaim for item in self.claims]
                scores = MethodFactory.create(method).compute(self.prompt, subclaims)
                for item, score in zip(self.claims, scores):
                    item.scores[method] = score
        else:
            subclaims = [item.subclaim for item in self.claims]
            scores = method.compute(self.prompt, subclaims)

        if noise:
            scores += self.noise

        if ranking:
            # Add noise to break ties
            rankings = rankdata(-scores, method='ordinal')
            return (len(scores) + 1 - rankings) / len(scores)
        else:
            return scores

    def get_accepted_subclaims(
            self, method: ConfidenceMethod, ranking: bool, threshold: float
    ) -> list[FactualSubclaim]:
        """
        Get the claims with a factuality score R(c) above the threshold.

        :param method: ConfidenceMethod to use as the subclaim scoring function.
        :param ranking: Whether to use ranking or not.
        :param threshold: Threshold for the confidence score.
        :return: List of accepted subclaims.
        """

        scores = self.get_confidence_score(method, ranking, noise=True)
        accepted_subclaims = [claim for claim, score in zip(self.claims, scores) if score >= threshold]
        return accepted_subclaims

    def get_conformal_score(self, method: ConfidenceMethod, ranking: bool, beta: float) -> float:
        """
        Compute the conformal score for a dataset instance when method is used as the subclaim scoring function.

        Full entailment: S(x·µ¢, y·µ¢) := inf { q ‚àà ‚Ñù‚Çä | ‚àÄq' ‚â• q, ‚àÄc ‚àà F‚Ççq'‚Çé(≈∑·µ¢), W(c, x·µ¢, y·µ¢) = 1 }.
                                   := inf { q ‚àà ‚Ñù‚Çä | ‚àÄq' ‚â• q, [‚àë‚Ççc‚ààF‚Ççq'‚Çé(≈∑·µ¢)‚Çé W(c, x·µ¢, y·µ¢) / |F‚Ççq'‚Çé(≈∑·µ¢)|] = 1}
        Partial entailment: S‚Çê(x·µ¢, y·µ¢) := inf { q ‚àà ‚Ñù‚Çä | ‚àÄq' ‚â• q, ùîº‚Ççc‚àºF‚Ççq'‚Çé(≈∑·µ¢)‚Çé [W(c, x·µ¢, y·µ¢)] ‚â• a }.

        This is equivalent to lowering the threshold until the beta of correct claims out of the selected claims
        is less than the beta, which is similar to precision.

        When beta = 1, the threshold is the largest threshold q such that all retained claims are correct.
        When beta = 0, the threshold is -inf as no factually correct claims are required to be retained.

        :param method: ConfidenceMethod to use as the subclaim scoring function.
        :param ranking: Whether to use ranking or not.
        :param beta: Fraction of correct subclaims required (a in formula above).
        :return: Conformal score for the dataset instance.
        """
        confidence_scores = self.get_confidence_score(method, ranking=ranking)

        sort_idx = np.argsort(-confidence_scores)  # sort in descending order
        sorted_confidence_scores = confidence_scores[sort_idx]

        correctness = self.labels[sort_idx]

        # Edge cases:
        # - All claims are correct (beta = 1): -inf
        # - All claims are wrong (beta = 1): highest confidence score
        # - No claims are required to be correct (beta = 0): -inf

        entailment_fractions = np.cumsum(correctness) / np.arange(1, len(correctness) + 1)

        # Find the first index where the entailment beta is less than the beta
        indices = np.asarray(entailment_fractions < beta).nonzero()[0]
        return float(sorted_confidence_scores[indices[0]] if indices.size > 0 else -np.inf)

        # Loop-based version
        # thresholds = sorted([(score + claim.noise) for score, claim in zip(scores, self.claims)], reverse=True)
        # for threshold in thresholds:
        #     accepted_subclaims = self.get_accepted_subclaims(method, ranking=ranking, threshold=threshold)
        #
        #     if accepted_subclaims:
        #         entailed_count = sum(1 for subclaim in accepted_subclaims if subclaim.label.is_correct())
        #         entailed_fraction = entailed_count / len(accepted_subclaims)
        #     else:
        #         entailed_fraction = 1
        #
        #     if entailed_fraction < beta:
        #         return threshold
        # return -math.inf


class FactualTaskManager(TaskManager):
    async def get_subclaims(
            self, text: str, sampling_params: Optional[SamplingParams] = None
    ) -> list[ScoredFactualClaim]:
        prompt = f"""
        Break down the following text into a set of small, independent claims that are easy to verify by a human.
        Each atomic fact should be a short, non-overlapping, verifiable statement that conveys a single piece of information.
        Atomic subclaims are more granular than full sentences and are used for fine-grained evaluation.

        text (The input text containing one or multiple claims): {text}
        """
        response = await self.generate_async(prompt, [
            FieldDefinition(
                name='subclaims', field_type=list[ScoredFactualClaim],
                description="List of independent atomic subclaims with confidence scores."
            )
        ], sampling_params=sampling_params)
        return response.subclaims

    async def score_subclaims(
            self, subclaims: list[str], sampling_params: Optional[ScoredFactualClaim] = None
    ) -> list[ScoredFactualClaim]:
        instruction = f"""
        Generate a list of scores for each claim indicating whether the claim is factual or not.

        """
        prompt = '\n'.join([instruction, 'subclaims (A list of subclaims)'] + subclaims)

        response = await self.generate_async(prompt, [
            FieldDefinition(
                name='scores', field_type=list[float],
                description="A list of confidence scores for each claim, where 1 is obvious subclaims and results like "
                            "'The earth is round' and '1+1=2'. A 0 is for claims that are very obscure or difficult "
                            "for anyone to know, like the birthdays of non-notable people."
            )
        ], sampling_params=sampling_params)

        scores = response.scores

        if len(scores) != len(subclaims):
            logger.info('Length of output scores does not match subclaims, falling back to individual queries')
            scores = []

            for claim in subclaims:
                prompt = '\n'.join([instruction, f'claim (A given subclaim): {claim}'])

                response = await self.generate_async(prompt, [
                    FieldDefinition(
                        name='score', field_type=float,
                        description="Confidence score in the claim, where 1 is obvious subclaims and results like "
                                    "'The earth is round' and '1+1=2'. A 0 is for claims that are very obscure or difficult "
                                    "for anyone to know, like the birthdays of non-notable people."
                    )
                ], sampling_params=sampling_params)
                scores.append(response.score)

        return [ScoredFactualClaim(claim=claim, confidence=score) for claim, score in zip(subclaims, scores)]

    async def get_relevance(
            self, subclaims: list[Union[str, ScoredFactualClaim]], text: str,
            sampling_params: Optional[SamplingParams] = None
    ) -> list[SupportScore]:
        # Override temperature until Enum support is enabled
        if sampling_params is None:
            sampling_params = SamplingParams(temperature=0.0)
        else:
            sampling_params = sampling_params.copy()
            sampling_params.temperature = 0.0

        instruction = f"""
        For each claim, score whether the text supports, contradicts, or is unrelated to the claim.

        Ensure the number of claims and scores are equal.

        text (An input text): {text}

        """
        subclaims = [item.claim if isinstance(item, ScoredFactualClaim) else item for item in subclaims]
        prompt = '\n'.join([instruction, 'subclaims (A list of subclaims)'] + subclaims)

        response = await self.generate_async(prompt, [
            FieldDefinition(
                name='scores', field_type=list[SupportScore],
                description="A list of scores indicating whether the text supports (1), "
                            "contradicts (-1), or is unrelated (0) to each subclaim."
            )
        ], sampling_params=sampling_params)

        scores = response.scores

        if len(scores) == len(subclaims):
            return scores

        logger.info('Length of output scores does not match subclaims, falling back to individual queries')
        scores = []

        for claim in subclaims:
            prompt = '\n'.join([instruction, f'claim (A given subclaim): {claim}'])

            response = await self.generate_async(prompt, [
                FieldDefinition(
                    name='score', field_type=SupportScore,
                    description="A score indicating whether the text supports (1), "
                                "contradicts (-1), or is unrelated (0) to the given subclaim."
                )
            ], sampling_params=sampling_params)
            scores.append(response.score)
        return scores

    async def get_output(
            self,
            prompt: str,
            subclaims: Optional[Iterable[Union[str, ScoredFactualClaim]]] = None,
            sampling_params: Optional[SamplingParams] = None
    ) -> str:
        if subclaims is None:
            instruction = f"""
            Generate an output based on the input prompt.
            
            prompt (The prompt to follow): {prompt}
            """
            response = await self.generate_async(instruction, sampling_params=sampling_params)
        else:
            instruction = f"""
            You will get a prompt and a set of subclaims that are true.
            Construct an answer using ONLY the subclaims provided, and try to use all subclaims as long as its possible.
            If no subclaims are given, reply to the prompt incorporating the fact that you don't know enough to fully respond.

            prompt (The prompt to follow): {prompt}

            subclaims (A list of subclaims that are factual)
            """

            subclaims = [claim.claim if isinstance(claim, ScoredFactualClaim) else claim for claim in subclaims]
            response = await self.generate_async('\n'.join([instruction] + subclaims), sampling_params=sampling_params)
        return response

    async def get_frequency_scores(
            self, subclaims: list[str], text: str, n_samples: int = 5,
            sampling_params: Optional[SamplingParams] = None
    ) -> list[float]:
        """
        Score each subclaim based on how frequently it appears in alternate outputs.
        """
        scores = [0] * len(subclaims)

        for _ in range(n_samples):
            output = await self.get_output(text, sampling_params=sampling_params)
            relevance = await self.get_relevance(subclaims, output, sampling_params=sampling_params)

            for i, score in enumerate(relevance):
                scores[i] += float(score.value)
        return scores

    async def get_instance(
            self,
            text: str,
            subclaims: Optional[list[str]] = None,
            sampling_params: Optional[SamplingParams] = None
    ):
        output = await self.get_output(text, sampling_params=sampling_params)
        if subclaims is None:
            subclaims = await self.get_subclaims(text, sampling_params=sampling_params)
        else:
            subclaims = await self.score_subclaims(subclaims, sampling_params=sampling_params)

        labels = [FactualAnnotation.UNKNOWN] * len(subclaims)

        claims = []
        for item, label in zip(subclaims, labels):
            claim = FactualSubclaim(
                subclaim=item.claim,
                scores={'gpt': item.confidence, 'optimal': label.is_important()},
                label=label,
            )
            claims.append(claim)

        return FactualInstance(**{
            'prompt': text,
            'original-output': output,
            'claims': claims
        })
